<!DOCTYPE html>
<html lang="zh-CN">
</div>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>FastAPI + Vosk 实时语音转文字</title>
    <style>
        body {
            font-family: sans-serif;
            padding: 20px;
            display: flex;
            flex-direction: column;
            min-height: 95vh;
            margin: 0;
        }

        h1 {
            text-align: center;
            margin-bottom: 20px;
            flex-shrink: 0;
        }

        .main-container {
            display: flex;
            flex-grow: 1;
            gap: 20px;
            width: 100%;
            max-width: 1400px;
            margin: 0 auto;
        }

        #digitalHumanArea {
            flex-basis: 65%;
            flex-shrink: 0;
            border: 2px dashed #ccc;
            min-height: 400px;
            display: flex;
            align-items: center;
            justify-content: center;
            color: #aaa;
            font-size: 1.2em;
            text-align: center;
            background-color: #f8f8f8;
            padding: 15px;
            box-sizing: border-box;
        }

        #speechInteractionWrapper {
            flex-basis: 35%;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            gap: 20px;
            min-width: 300px;
        }

        #speechRecognitionArea, #controls-file {
            padding: 20px;
            border: 1px solid #ddd;
            border-radius: 8px;
            background-color: #fff;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
            width: 100%;
            box-sizing: border-box;
            display: flex;
            flex-direction: column;
            height: 100%;
        }

        #controls {
            display: flex;
            align-items: center;
            gap: 10px;
            margin-bottom: 15px;
        }

        #controls button {
            padding: 10px 15px;
            font-size: 16px;
            cursor: pointer;
        }

        #status-mic {
            margin-left: auto;
            font-style: italic;
            color: #555;
        }

        .upload-actions {
            display: flex;
            gap: 8px;
            margin-bottom: 10px;
        }

        .upload-actions input[type="file"],
        .upload-actions button {
            max-width: 100%;
            box-sizing: border-box;
        }

        .upload-status {
            margin-top: 10px;
            font-size: 0.9em;
            color: #555;
            word-break: break-all;
        }

        #file-info {
            margin-right: 10px;
            display: inline-block;
        }

        #transcript {
            margin-top: 10px;
            padding: 15px;
            border: 1px solid #ccc;
            min-height: 100px;
            height: calc(100% - 150px);
            overflow-y: auto;
            background-color: #f9f9f9;
            white-space: pre-wrap;
            border-radius: 4px;
            line-height: 1.6;
            flex-grow: 1;
        }

        #transcript p:first-child {
            color: #888;
            font-style: italic;
            text-align: center;
        }

        .partial {
            color: #888;
            opacity: 0.8;
        }

        .section-title {
            font-weight: bold;
            margin-bottom: 10px;
            display: block;
        }

        .status {
            font-style: italic;
            color: #555;
        }
    </style>
</head>
<body>

<h1>实时语音转文字与数字人交互</h1>

<div class="main-container">
    <!-- 左侧：数字人区域 -->
    <div id="digitalHumanArea">
        这里是预留的数字人显示区域 (65%)
        <!-- <img src="your-image.jpg" alt="数字人" style="max-width: 100%; max-height: 100%;"> -->
    </div>

    <!-- 右侧：语音交互区域 -->
    <div id="speechInteractionWrapper">
        <!-- 麦克风识别 -->
        <div id="speechRecognitionArea">
            <div id="controls">
                <button id="startButton">开始识别</button>
                <button id="stopButton" disabled>停止识别</button>
                <span id="status-mic">状态：未连接</span>
            </div>

            <div class="upload-actions">
                <input type="file" id="audioFileInput" accept="audio/*">
                <button id="uploadButton">上传并识别</button>
            </div>
            
            <div class="upload-status">
                <span id="file-info"></span>
                <span id="status-file" class="status"></span>
            </div>
           
            <div id="transcript">
                <p>识别结果将显示在这里...</p>
            </div>
        </div>
    </div>
</div>


<script>
    // JavaScript 部分保持不变，因为它通过 ID 选择元素，
    // 并且布局的改变不影响其功能逻辑。
    const startButton = document.getElementById('startButton');
    const stopButton = document.getElementById('stopButton');
    const statusDiv = document.getElementById('status-mic');
    const audioFileInput = document.getElementById('audioFileInput');
    const uploadButton = document.getElementById('uploadButton');
    const statusFile = document.getElementById('status-file');
    const fileInfo = document.getElementById('file-info');

    const transcriptDiv = document.getElementById('transcript');

    let websocket;
    let audioContext;
    let scriptProcessor; // 或者 AudioWorkletNode
    let mediaStream;
    let isRecording = false;
    let partialTranscriptElement = null; // 用于跟踪当前的部分结果元素

    // --- WebSocket 地址 ---
    const wsUrl = `ws://${window.location.host}/ws`;

    // --- 音频参数 ---
    const TARGET_SAMPLE_RATE = 16000;
    const BUFFER_SIZE = 4096;

    // --- 开始识别 ---
    startButton.onclick = async () => {
        if (isRecording) return;
        statusDiv.textContent = '状态：正在请求麦克风权限...';
        // 清空转录区域，保留初始提示结构
        transcriptDiv.innerHTML = '<p>识别结果将显示在这里...</p>';
        partialTranscriptElement = null;

        try {
            mediaStream = await navigator.mediaDevices.getUserMedia({
                audio: {
                    echoCancellation: true,
                    noiseSuppression: true,
                },
                video: false
            });

            statusDiv.textContent = '状态：已获取麦克风权限，正在连接服务器...';

            websocket = new WebSocket(wsUrl);

            websocket.onopen = () => {
                statusDiv.textContent = '状态：连接成功，正在识别...';
                isRecording = true;
                startButton.disabled = true;
                stopButton.disabled = false;
                startAudioProcessing(); // 开始处理音频
            };

            websocket.onmessage = (event) => {
                try {
                    const result = JSON.parse(event.data);
                    if (result.error) {
                        console.error("服务器错误:", result.error);
                        statusDiv.textContent = `状态：错误 - ${result.error}`;
                        stopRecording();
                        return;
                    }
                    updateTranscript(result);
                } catch (e) {
                    console.error("解析服务器消息失败:", e);
                }
            };

            websocket.onerror = (event) => {
                console.error("WebSocket 错误:", event);
                statusDiv.textContent = '状态：WebSocket 连接错误';
                isRecording = false;
                cleanupAudio();
                startButton.disabled = false;
                stopButton.disabled = true;
            };

            websocket.onclose = (event) => {
                console.log("WebSocket 连接关闭:", event.code, event.reason);
                if (isRecording) { // 避免在手动停止时显示“已断开”
                    statusDiv.textContent = '状态：连接已断开';
                }
                isRecording = false;
                cleanupAudio();
                startButton.disabled = false;
                stopButton.disabled = true;
                partialTranscriptElement = null;
            };

        } catch (err) {
            console.error('获取麦克风或连接 WebSocket 失败:', err);
            statusDiv.textContent = `状态：错误 - ${err.message}`;
            cleanupAudio();
            isRecording = false;
            startButton.disabled = false;
            stopButton.disabled = true;
        }
    };

    // --- 停止识别 ---
    stopButton.onclick = () => {
        stopRecording();
    };

    audioFileInput.onchange = () => {
        const file = audioFileInput.files[0];
        if (file) {
            fileInfo.textContent = `已选择: ${file.name} (${(file.size / 1024 / 1024).toFixed(2)} MB)`;
             statusFile.textContent = ''; // Clear previous status/error
             statusFile.classList.remove('error');
        } else {
            fileInfo.textContent = '';
        }
    };

    uploadButton.onclick = async () => {
        const file = audioFileInput.files[0];
        if (!file) {
            alert('请先选择一个音频文件！');
            return;
        }

        const formData = new FormData();
        formData.append('audio_file', file);

        // Disable buttons during upload
        uploadButton.disabled = true;
        audioFileInput.disabled = true;
        statusFile.textContent = '状态: 正在上传和处理...';
        statusFile.classList.remove('error');
        // Clear previous output OR indicate new process starting
        transcriptDiv.textContent = '正在处理上传的文件...'; // Indicate processing

        try {
            const response = await fetch('/upload_audio/', {
                method: 'POST',
                body: formData
            });

            if (!response.ok) {
                let errorMsg = `HTTP 错误: ${response.status}`;
                try { // Try to get detailed error from server response body
                    const errData = await response.json();
                    errorMsg += ` - ${errData.detail || '未知错误'}`;
                } catch (e) { /* Ignore if response body is not JSON */ }
                throw new Error(errorMsg);
            }

            const data = await response.json();

            // ** 更新统一的输出区域 **
            // Replace the content with the file's transcription result
            transcriptDiv.textContent = `文件 (${file.name}) 识别结果:\n--------------------------\n${data.transcription}`;
            statusFile.textContent = '状态: 处理完成！';
            console.log('File transcription:', data.transcription);

        } catch (error) {
            console.error('Error uploading or processing file:', error);
            statusFile.textContent = `错误: ${error.message}`;
             statusFile.classList.add('error');
             // Show error in the main output area as well
             transcriptDiv.textContent = `文件处理出错: ${error.message}`;
        } finally {
            // Re-enable buttons
            uploadButton.disabled = false;
            audioFileInput.disabled = false;
            // Optionally clear file input for next upload?
            // audioFileInput.value = null;
            // fileInfo.textContent = '';
        }
    };

    // --- 停止录音和连接的函数 ---
    function stopRecording() {
        if (!isRecording) return;
        isRecording = false;
        statusDiv.textContent = '状态：已停止';
        startButton.disabled = false;
        stopButton.disabled = true;

        if (websocket && websocket.readyState === WebSocket.OPEN) {
            // 可以在关闭前发送一个特殊的“结束”信号（如果后端需要）
            // websocket.send(JSON.stringify({ command: "stop" }));
            websocket.close();
        }
        websocket = null;

        cleanupAudio();
    }

    // --- 开始处理音频 (仍使用 ScriptProcessorNode) ---
    function startAudioProcessing() {
        audioContext = new (window.AudioContext || window.webkitAudioContext)();

        const sourceSampleRate = audioContext.sampleRate;
        console.log(`音频上下文采样率: ${sourceSampleRate} Hz`);

        // 建议在实际使用中切换到 AudioWorklet
        if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
             console.error("浏览器不支持 getUserMedia");
             statusDiv.textContent = '状态：错误 - 浏览器不支持麦克风输入';
             stopRecording();
             return;
        }

        const source = audioContext.createMediaStreamSource(mediaStream);

        // 注意: ScriptProcessorNode 已被废弃
        scriptProcessor = audioContext.createScriptProcessor(BUFFER_SIZE, 1, 1);

        scriptProcessor.onaudioprocess = (event) => {
            if (!isRecording || !websocket || websocket.readyState !== WebSocket.OPEN) {
                return;
            }

            const inputData = event.inputBuffer.getChannelData(0);
            const pcm16Buffer = convertFloat32ToInt16(inputData, sourceSampleRate, TARGET_SAMPLE_RATE);
            websocket.send(pcm16Buffer);
        };

        source.connect(scriptProcessor);
        scriptProcessor.connect(audioContext.destination); // 必需，否则可能不触发 process
    }

    // --- 清理音频资源 ---
    function cleanupAudio() {
        if (scriptProcessor) {
            scriptProcessor.disconnect();
            scriptProcessor.onaudioprocess = null;
            scriptProcessor = null;
        }
        // 检查 audioContext 状态，避免在已关闭时再次关闭
        if (audioContext && audioContext.state !== 'closed') {
            audioContext.close().catch(e => console.warn("关闭 AudioContext 时出错:", e));
        }
        audioContext = null; // 清除引用
        if (mediaStream) {
            mediaStream.getTracks().forEach(track => track.stop());
            mediaStream = null;
        }
        console.log("音频资源已清理");
    }

   // --- 更新转录文本显示 ---
    function updateTranscript(result) {
        // 查找初始的 <p> 元素
        const initialPrompt = transcriptDiv.querySelector('p');
        // 如果初始提示信息仍然存在，则移除它
        if (initialPrompt && initialPrompt.textContent === '识别结果将显示在这里...') {
            transcriptDiv.innerHTML = ''; // 清空内容，准备接收真实转录
        }


        if (result.partial) {
            const partialText = result.partial.trim();
             // 如果部分结果为空（可能在静音段后），不做任何操作或移除部分元素
            if (!partialText) {
                if (partialTranscriptElement) {
                    partialTranscriptElement.remove();
                    partialTranscriptElement = null;
                }
                return; // 没有部分文本，直接返回
            }

            if (!partialTranscriptElement) {
                partialTranscriptElement = document.createElement('span');
                partialTranscriptElement.className = 'partial';
                // 将部分结果添加到最后，如果前面有最终结果，则在其后添加
                transcriptDiv.appendChild(partialTranscriptElement);
                 // 添加一个空格，使其与之前的最终结果分开
                if(transcriptDiv.childElementCount > 1) { // 检查是否是第一个元素
                     partialTranscriptElement.insertAdjacentText('beforebegin', ' ');
                }
            }
            partialTranscriptElement.textContent = partialText + '...'; // 显示部分结果

        } else if (result.text && result.text.trim()) {
            const finalText = result.text.trim();
            let finalSpan;

            if (partialTranscriptElement) {
                // 如果存在部分结果，用最终结果替换它
                finalSpan = document.createElement('span');
                finalSpan.textContent = finalText + '。'; // 加上句号
                partialTranscriptElement.replaceWith(finalSpan); // 替换节点
                partialTranscriptElement = null; // 清除引用
            } else {
                // 如果没有部分结果，直接添加最终结果
                finalSpan = document.createElement('span');
                finalSpan.textContent = finalText + '。'; // 加上句号
                // 在添加前检查最后一个元素是否是 span，如果是，则在其后加一个空格再添加新的
                if(transcriptDiv.lastElementChild && transcriptDiv.lastElementChild.tagName === 'SPAN') {
                    transcriptDiv.appendChild(document.createTextNode(' ')); // 添加空格文本节点
                }
                transcriptDiv.appendChild(finalSpan);
            }
        }
         // 始终滚动到底部以显示最新内容
        transcriptDiv.scrollTop = transcriptDiv.scrollHeight;
    }


    // --- 音频格式转换和重采样函数 ---
    function convertFloat32ToInt16(buffer, sourceSr, targetSr) {
        let outputBuffer;
        let sampleRatio = sourceSr / targetSr;
        let newLength = Math.round(buffer.length / sampleRatio);
        outputBuffer = new Int16Array(newLength);
        let offsetResult = 0;
        let offsetBuffer = 0;

        while (offsetResult < newLength) {
            let nextOffsetBuffer = Math.round((offsetResult + 1) * sampleRatio);
            let accum = 0, count = 0;
            for (let i = offsetBuffer; i < nextOffsetBuffer && i < buffer.length; i++) {
                accum += buffer[i];
                count++;
            }
            // Clamp and convert value
            let value = Math.max(-1, Math.min(1, count > 0 ? accum / count : 0));
             // Convert to 16-bit PCM
            outputBuffer[offsetResult] = value < 0 ? value * 0x8000 : value * 0x7FFF;
            offsetResult++;
            offsetBuffer = nextOffsetBuffer;
        }
        return outputBuffer.buffer; // 返回 ArrayBuffer
    }

</script>

</body>
</html>